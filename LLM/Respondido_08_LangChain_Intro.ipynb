{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Alunos:\n",
        "\n",
        "- Bruno Santos (bfss@cesar.school)\n",
        "- Caio Guedes (ccsg@cesar.school)"
      ],
      "metadata": {
        "id": "O4r57GTUiM5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai langchain"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qJ_gtbpY0k_",
        "outputId": "a64e25fb-d927-4364-9b07-69e1f5b618be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.29)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.59.6)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_openai) (3.0.0)\n",
            "Downloading langchain_openai-0.3.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.3.0 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n-cainv7V5Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25cfd57-1db6-4705-c253-01ea750d3c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P46Zl2-iV5Um"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WZ4gLZXjV5Un"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MJCBl34RV5Uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e28c36-effe-46f9-9eec-d0198f0af122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá! Como posso ajudar você hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-2720257f-3889-4df1-b815-4a4b2a38066c-0' usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Olá\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw3-kUckV5Up"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIMYD3ThV5Uq"
      },
      "source": [
        "### Templates Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M4QQ9iC0V5Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19171174-cb4a-44ad-a450-469ebf91d1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para português: Artificial Intelligence is the future!', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para português: {texto}\")\n",
        "\n",
        "prompt = prompt_template.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WYqHBo6sV5Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed1d9ab-e5cb-40d7-a0ae-0d6b25db91f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Inteligência Artificial é o futuro!\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwmMg8jIV5Us"
      },
      "source": [
        "### Templates de Mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nkyV8w8RV5Us",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c741cc2-ef59-4f04-9521-41b124e34c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá, como você está?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 36, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-b0ec8f0b-2ecd-4089-b774-64d559e29e9a-0' usage_metadata={'input_tokens': 36, 'output_tokens': 7, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hjB3VA6AV5Ut"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xKaYfSbkV5Ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a409c0-2b41-4868-9676-afdeedb71687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"português\",\n",
        "    \"texto\": \"Hello, how are you?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V5sgIfCAV5Ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759eeb14-e5d5-4c1e-e778-1ee73c63e743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá, como você está?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p04xV2xjV5Uu"
      },
      "source": [
        "### Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MY4zAjwoV5Uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce675717-6ea8-43d0-9f76-a1e55da14639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='A capital do Rio Grande do Norte é Natal.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 16, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-8a57f8ac-83ec-4373-9e75-c60c90f2ae1e-0' usage_metadata={'input_tokens': 16, 'output_tokens': 11, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Saída do parser:\n",
            "A capital do Rio Grande do Norte é Natal.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "str_parser = StrOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Qual a capital do Rio Grande do Norte?\")\n",
        "output = str_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Saída do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PUdUwGaXV5Uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a73991-a023-4f6f-839b-193f61a9587e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='Aqui está a representação das massas e cargas das partículas que constituem o átomo no formato JSON:\\n\\n```json\\n{\\n  \"próton\": {\\n    \"massa\": \"1,67 × 10^-27 kg\",\\n    \"carga\": \"+1 e\"\\n  },\\n  \"nêutron\": {\\n    \"massa\": \"1,68 × 10^-27 kg\",\\n    \"carga\": \"0 e\"\\n  },\\n  \"elétron\": {\\n    \"massa\": \"9,11 × 10^-31 kg\",\\n    \"carga\": \"-1 e\"\\n  }\\n}\\n```\\n\\nNeste JSON, as massas estão expressas em quilogramas (kg) e as cargas estão representadas em unidades de carga elementar (e).' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 157, 'prompt_tokens': 38, 'total_tokens': 195, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8221799169', 'finish_reason': 'stop', 'logprobs': None} id='run-84f4b3f5-ddb2-4e91-8510-15ad5ca002fd-0' usage_metadata={'input_tokens': 38, 'output_tokens': 157, 'total_tokens': 195, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Saída do parser:\n",
            "{'próton': {'massa': '1,67 × 10^-27 kg', 'carga': '+1 e'}, 'nêutron': {'massa': '1,68 × 10^-27 kg', 'carga': '0 e'}, 'elétron': {'massa': '9,11 × 10^-31 kg', 'carga': '-1 e'}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Quais as massas e cargas das partículas que constituem o átomo? Responda no formato JSON em que cada chave seja o nome da partícula\")\n",
        "output = json_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Saída do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwUTex9rV5Uu"
      },
      "source": [
        "## Encadeamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RgXOoxl6V5Uv"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "THhSGJeJV5Uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c88d567-6686-4e07-eac7-ae70da122e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubarões!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ToVIWnDLV5Uv"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "52AQoT-LV5Uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f882ad4-dc3b-4051-9f7f-e985b99736cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"inglês\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMgA2t7WV5Uv"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf25jNvBV5Uv"
      },
      "source": [
        "### Exercício 1\n",
        "Crie uma `chain` que a partir de um tópico informado pelo usuário, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmNyncNGelbE",
        "outputId": "812b70ef-2bf2-4898-bb3f-63b12d6b14dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por que o cavalo foi ao bar?\n",
            "\n",
            "Porque ele queria tomar uma \"cerveja de alfalfa\"! 🐴🍺\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"Conte uma piada engraçada sobre: {tema}\")\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "response = chain.invoke({\n",
        "    \"tema\": \"cavalo\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JatgX7bHV5Ux"
      },
      "source": [
        "### Exercício 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Z74SRbXeV5Ux",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f1cea131-b503-4046-bfa2-e34570dab56b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negativo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "prompt_code = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Faça uma classificação de sentimento do texto a seguir. O resultado deve ser: Positivo, Neutro ou Negativo.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "chain = prompt_code | llm | StrOutputParser()\n",
        "\n",
        "chain.invoke({\n",
        "    \"texto\": \"Odiei o hotel em que ficamos, nota 2.\"\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R5xzeZbV5Ux"
      },
      "source": [
        "### Exercício 3\n",
        "Crie uma `chain` que gere o código de uma função Python de acordo com a descrição do usuário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aGZQ3VphV5Uy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d81edd13-f39e-4ec9-f940-81c7c6b3f59e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Claro! Aqui está um código Python que calcula quantos dias existem entre a data atual e uma data de entrada fornecida pelo usuário no formato DD/MM/YYYY:\\n\\n```python\\nfrom datetime import datetime\\n\\ndef calcular_dias_ate_data(data_str):\\n    # Formato da data que será recebido\\n    formato = \"%d/%m/%Y\"\\n    \\n    try:\\n        # Converte a string em um objeto datetime\\n        data_entrada = datetime.strptime(data_str, formato)\\n        \\n        # Obtém a data atual\\n        data_atual = datetime.now()\\n        \\n        # Calcula a diferença em dias\\n        diferenca = (data_entrada - data_atual).days\\n        \\n        return diferenca\\n    except ValueError:\\n        return \"Data inválida. Por favor, use o formato DD/MM/YYYY.\"\\n\\ndef main():\\n    # Solicita a data de entrada ao usuário\\n    data_str = input(\"Digite a data (DD/MM/YYYY): \")\\n    \\n    # Calcula e exibe a diferença em dias\\n    dias = calcular_dias_ate_data(data_str)\\n    print(f\"Dias até a data informada: {dias}\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Como funciona:\\n1. O usuário é solicitado a inserir uma data no formato DD/MM/YYYY.\\n2. O programa tenta converter essa string para um objeto `datetime`.\\n3. A diferença em dias entre a data de entrada e a data atual é calculada e exibida.\\n4. Se o formato da data estiver incorreto, uma mensagem de erro é exibida.\\n\\n### Instruções para execução:\\n- Certifique-se de ter o Python instalado no seu sistema.\\n- Salve o código em um arquivo com a extensão `.py`, por exemplo, `calculo_dias.py`.\\n- Execute o arquivo a partir do terminal ou console com o comando: `python calculo_dias.py` e siga as instruções na tela.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "prompt_code = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Gere um código Python bem formatado e funcional sobre o pedido a seguir.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "chain = prompt_code | llm | StrOutputParser()\n",
        "\n",
        "chain.invoke({\n",
        "    \"texto\": \"Calcule quantos dias de hoje até a data de entrada no formato DD/MM/YYYY\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def calcular_dias_ate_data(data_str):\n",
        "    # Formato da data que será recebido\n",
        "    formato = \"%d/%m/%Y\"\n",
        "\n",
        "    try:\n",
        "        # Converte a string em um objeto datetime\n",
        "        data_entrada = datetime.strptime(data_str, formato)\n",
        "\n",
        "        # Obtém a data atual\n",
        "        data_atual = datetime.now()\n",
        "\n",
        "        # Calcula a diferença em dias\n",
        "        diferenca = (data_entrada - data_atual).days\n",
        "\n",
        "        return diferenca\n",
        "    except ValueError:\n",
        "        return \"Data inválida. Por favor, use o formato DD/MM/YYYY.\"\n",
        "\n",
        "def main():\n",
        "    # Solicita a data de entrada ao usuário\n",
        "    data_str = input(\"Digite a data (DD/MM/YYYY): \")\n",
        "\n",
        "    # Calcula e exibe a diferença em dias\n",
        "    dias = calcular_dias_ate_data(data_str)\n",
        "    print(f\"Dias até a data informada: {dias}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2h44CuzggL5",
        "outputId": "c85c2229-e54f-4027-c5c4-26aaaa773c46"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite a data (DD/MM/YYYY): 08/10/2024\n",
            "Dias até a data informada: -101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJlMQ9XrV5Uy"
      },
      "source": [
        "### Exercício 4\n",
        "Crie uma `chain` que explique de forma simplificada um tópico geral fornecido pelo usuário e, em seguida, traduza a explicação para inglês. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uqA53-98V5Uy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c08ae96-0797-416b-bddb-6432eefd835b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glucose is a type of simple sugar, or monosaccharide, that serves as an important source of energy for the human body. It is essential for the functioning of cells, especially those in the brain. Glucose is primarily obtained through the digestion of carbohydrates, which are broken down into simple sugars. The level of glucose in the blood, known as glycemia, is regulated by hormones such as insulin and glucagon. Elevated glucose levels may indicate diabetes, while very low levels can cause hypoglycemia, both of which can be harmful to health.\n"
          ]
        }
      ],
      "source": [
        "prompt_explicacao = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Me explique sobre o assunto de forma resumida\"),\n",
        "        (\"user\", \"{topico_geral}\")\n",
        "    ]\n",
        ")\n",
        "prompt_traducao = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é perfeitamente fluente em inglês e preciso que traduza esse texto para mim.\"),\n",
        "        (\"user\", \"{topico_geral}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_explicacao | llm | StrOutputParser() | prompt_traducao | llm | StrOutputParser()\n",
        "print(chain.invoke({\"topico_geral\": \"glicose\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6eq6BV6V5Uz"
      },
      "source": [
        "### Exercício 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "HY4s4j8NV5Uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e57942-2ca9-4d61-ab8f-7252db6879e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O Cesar School oferece diversos cursos focados em tecnologia e inovação. Alguns dos principais cursos incluem:\n",
            "\n",
            "1. **Cursos de Pós-Graduação**:\n",
            "   - MBA em Gestão de Projetos\n",
            "   - MBA em Inovação\n",
            "   - MBA em Marketing Digital\n",
            "   - Mestrado em Engenharia de Computação\n",
            "   - Especialização em Desenvolvimento de Software\n",
            "\n",
            "2. **Cursos de Extensão**:\n",
            "   - Cursos de capacitação em áreas como Design, Dados, Inteligência Artificial, Transformação Digital e Programação.\n",
            "   - Workshops e eventos para atualização profissional.\n",
            "\n",
            "3. **Cursos de Formação**:\n",
            "   - Formação em áreas específicas, como Desenvolvimento Web, UX/UI Design, Data Science, entre outros.\n",
            "\n",
            "Além disso, o Cesar School promove programas de formação continuada e customizada para empresas e organizações, visando atender às demandas do mercado.\n",
            "\n",
            "Para informações mais detalhadas e atualizadas, é recomendável visitar o site oficial do Cesar School.\n"
          ]
        }
      ],
      "source": [
        "prompt_respostas_cesar = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um especialista no Cesar School, de Recife. Responda as perguntas feitas sobre o Cesar.\"),\n",
        "        (\"user\"), \"{pergunta}\"\n",
        "    ]\n",
        ")\n",
        "chain = prompt_respostas_cesar | llm | StrOutputParser()\n",
        "print(chain.invoke({\"pergunta\": \"Quais cursos tem no Cesar?\"}))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}