{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Alunos:\n",
        "\n",
        "- Bruno Santos (bfss@cesar.school)\n",
        "- Caio Guedes (ccsg@cesar.school)"
      ],
      "metadata": {
        "id": "O4r57GTUiM5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai langchain"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qJ_gtbpY0k_",
        "outputId": "a64e25fb-d927-4364-9b07-69e1f5b618be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.29)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.59.6)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_openai) (3.0.0)\n",
            "Downloading langchain_openai-0.3.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.3.0 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n-cainv7V5Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25cfd57-1db6-4705-c253-01ea750d3c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P46Zl2-iV5Um"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WZ4gLZXjV5Un"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MJCBl34RV5Uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e28c36-effe-46f9-9eec-d0198f0af122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Ol√°! Como posso ajudar voc√™ hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-2720257f-3889-4df1-b815-4a4b2a38066c-0' usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Ol√°\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw3-kUckV5Up"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIMYD3ThV5Uq"
      },
      "source": [
        "### Templates Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M4QQ9iC0V5Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19171174-cb4a-44ad-a450-469ebf91d1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para portugu√™s: Artificial Intelligence is the future!', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para portugu√™s: {texto}\")\n",
        "\n",
        "prompt = prompt_template.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WYqHBo6sV5Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed1d9ab-e5cb-40d7-a0ae-0d6b25db91f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Intelig√™ncia Artificial √© o futuro!\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwmMg8jIV5Us"
      },
      "source": [
        "### Templates de Mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nkyV8w8RV5Us",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c741cc2-ef59-4f04-9521-41b124e34c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Ol√°, como voc√™ est√°?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 36, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-b0ec8f0b-2ecd-4089-b774-64d559e29e9a-0' usage_metadata={'input_tokens': 36, 'output_tokens': 7, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hjB3VA6AV5Ut"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Voc√™ √© um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xKaYfSbkV5Ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a409c0-2b41-4868-9676-afdeedb71687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"ingl√™s\",\n",
        "    \"lingua_destino\": \"portugu√™s\",\n",
        "    \"texto\": \"Hello, how are you?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V5sgIfCAV5Ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759eeb14-e5d5-4c1e-e778-1ee73c63e743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ol√°, como voc√™ est√°?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p04xV2xjV5Uu"
      },
      "source": [
        "### Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MY4zAjwoV5Uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce675717-6ea8-43d0-9f76-a1e55da14639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='A capital do Rio Grande do Norte √© Natal.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 16, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-8a57f8ac-83ec-4373-9e75-c60c90f2ae1e-0' usage_metadata={'input_tokens': 16, 'output_tokens': 11, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Sa√≠da do parser:\n",
            "A capital do Rio Grande do Norte √© Natal.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "str_parser = StrOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Qual a capital do Rio Grande do Norte?\")\n",
        "output = str_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Sa√≠da do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PUdUwGaXV5Uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a73991-a023-4f6f-839b-193f61a9587e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='Aqui est√° a representa√ß√£o das massas e cargas das part√≠culas que constituem o √°tomo no formato JSON:\\n\\n```json\\n{\\n  \"pr√≥ton\": {\\n    \"massa\": \"1,67 √ó 10^-27 kg\",\\n    \"carga\": \"+1 e\"\\n  },\\n  \"n√™utron\": {\\n    \"massa\": \"1,68 √ó 10^-27 kg\",\\n    \"carga\": \"0 e\"\\n  },\\n  \"el√©tron\": {\\n    \"massa\": \"9,11 √ó 10^-31 kg\",\\n    \"carga\": \"-1 e\"\\n  }\\n}\\n```\\n\\nNeste JSON, as massas est√£o expressas em quilogramas (kg) e as cargas est√£o representadas em unidades de carga elementar (e).' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 157, 'prompt_tokens': 38, 'total_tokens': 195, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8221799169', 'finish_reason': 'stop', 'logprobs': None} id='run-84f4b3f5-ddb2-4e91-8510-15ad5ca002fd-0' usage_metadata={'input_tokens': 38, 'output_tokens': 157, 'total_tokens': 195, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Sa√≠da do parser:\n",
            "{'pr√≥ton': {'massa': '1,67 √ó 10^-27 kg', 'carga': '+1 e'}, 'n√™utron': {'massa': '1,68 √ó 10^-27 kg', 'carga': '0 e'}, 'el√©tron': {'massa': '9,11 √ó 10^-31 kg', 'carga': '-1 e'}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Quais as massas e cargas das part√≠culas que constituem o √°tomo? Responda no formato JSON em que cada chave seja o nome da part√≠cula\")\n",
        "output = json_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Sa√≠da do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwUTex9rV5Uu"
      },
      "source": [
        "## Encadeamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RgXOoxl6V5Uv"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "THhSGJeJV5Uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c88d567-6686-4e07-eac7-ae70da122e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬°Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"ingl√™s\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubar√µes!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ToVIWnDLV5Uv"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "52AQoT-LV5Uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f882ad4-dc3b-4051-9f7f-e985b99736cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬°Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"ingl√™s\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMgA2t7WV5Uv"
      },
      "source": [
        "## Exerc√≠cios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf25jNvBV5Uv"
      },
      "source": [
        "### Exerc√≠cio 1\n",
        "Crie uma `chain` que a partir de um t√≥pico informado pelo usu√°rio, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmNyncNGelbE",
        "outputId": "812b70ef-2bf2-4898-bb3f-63b12d6b14dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por que o cavalo foi ao bar?\n",
            "\n",
            "Porque ele queria tomar uma \"cerveja de alfalfa\"! üê¥üç∫\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"Conte uma piada engra√ßada sobre: {tema}\")\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "response = chain.invoke({\n",
        "    \"tema\": \"cavalo\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JatgX7bHV5Ux"
      },
      "source": [
        "### Exerc√≠cio 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Z74SRbXeV5Ux",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f1cea131-b503-4046-bfa2-e34570dab56b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negativo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "prompt_code = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Fa√ßa uma classifica√ß√£o de sentimento do texto a seguir. O resultado deve ser: Positivo, Neutro ou Negativo.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "chain = prompt_code | llm | StrOutputParser()\n",
        "\n",
        "chain.invoke({\n",
        "    \"texto\": \"Odiei o hotel em que ficamos, nota 2.\"\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R5xzeZbV5Ux"
      },
      "source": [
        "### Exerc√≠cio 3\n",
        "Crie uma `chain` que gere o c√≥digo de uma fun√ß√£o Python de acordo com a descri√ß√£o do usu√°rio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aGZQ3VphV5Uy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d81edd13-f39e-4ec9-f940-81c7c6b3f59e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Claro! Aqui est√° um c√≥digo Python que calcula quantos dias existem entre a data atual e uma data de entrada fornecida pelo usu√°rio no formato DD/MM/YYYY:\\n\\n```python\\nfrom datetime import datetime\\n\\ndef calcular_dias_ate_data(data_str):\\n    # Formato da data que ser√° recebido\\n    formato = \"%d/%m/%Y\"\\n    \\n    try:\\n        # Converte a string em um objeto datetime\\n        data_entrada = datetime.strptime(data_str, formato)\\n        \\n        # Obt√©m a data atual\\n        data_atual = datetime.now()\\n        \\n        # Calcula a diferen√ßa em dias\\n        diferenca = (data_entrada - data_atual).days\\n        \\n        return diferenca\\n    except ValueError:\\n        return \"Data inv√°lida. Por favor, use o formato DD/MM/YYYY.\"\\n\\ndef main():\\n    # Solicita a data de entrada ao usu√°rio\\n    data_str = input(\"Digite a data (DD/MM/YYYY): \")\\n    \\n    # Calcula e exibe a diferen√ßa em dias\\n    dias = calcular_dias_ate_data(data_str)\\n    print(f\"Dias at√© a data informada: {dias}\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Como funciona:\\n1. O usu√°rio √© solicitado a inserir uma data no formato DD/MM/YYYY.\\n2. O programa tenta converter essa string para um objeto `datetime`.\\n3. A diferen√ßa em dias entre a data de entrada e a data atual √© calculada e exibida.\\n4. Se o formato da data estiver incorreto, uma mensagem de erro √© exibida.\\n\\n### Instru√ß√µes para execu√ß√£o:\\n- Certifique-se de ter o Python instalado no seu sistema.\\n- Salve o c√≥digo em um arquivo com a extens√£o `.py`, por exemplo, `calculo_dias.py`.\\n- Execute o arquivo a partir do terminal ou console com o comando: `python calculo_dias.py` e siga as instru√ß√µes na tela.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "prompt_code = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Gere um c√≥digo Python bem formatado e funcional sobre o pedido a seguir.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "chain = prompt_code | llm | StrOutputParser()\n",
        "\n",
        "chain.invoke({\n",
        "    \"texto\": \"Calcule quantos dias de hoje at√© a data de entrada no formato DD/MM/YYYY\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def calcular_dias_ate_data(data_str):\n",
        "    # Formato da data que ser√° recebido\n",
        "    formato = \"%d/%m/%Y\"\n",
        "\n",
        "    try:\n",
        "        # Converte a string em um objeto datetime\n",
        "        data_entrada = datetime.strptime(data_str, formato)\n",
        "\n",
        "        # Obt√©m a data atual\n",
        "        data_atual = datetime.now()\n",
        "\n",
        "        # Calcula a diferen√ßa em dias\n",
        "        diferenca = (data_entrada - data_atual).days\n",
        "\n",
        "        return diferenca\n",
        "    except ValueError:\n",
        "        return \"Data inv√°lida. Por favor, use o formato DD/MM/YYYY.\"\n",
        "\n",
        "def main():\n",
        "    # Solicita a data de entrada ao usu√°rio\n",
        "    data_str = input(\"Digite a data (DD/MM/YYYY): \")\n",
        "\n",
        "    # Calcula e exibe a diferen√ßa em dias\n",
        "    dias = calcular_dias_ate_data(data_str)\n",
        "    print(f\"Dias at√© a data informada: {dias}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2h44CuzggL5",
        "outputId": "c85c2229-e54f-4027-c5c4-26aaaa773c46"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite a data (DD/MM/YYYY): 08/10/2024\n",
            "Dias at√© a data informada: -101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJlMQ9XrV5Uy"
      },
      "source": [
        "### Exerc√≠cio 4\n",
        "Crie uma `chain` que explique de forma simplificada um t√≥pico geral fornecido pelo usu√°rio e, em seguida, traduza a explica√ß√£o para ingl√™s. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uqA53-98V5Uy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c08ae96-0797-416b-bddb-6432eefd835b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glucose is a type of simple sugar, or monosaccharide, that serves as an important source of energy for the human body. It is essential for the functioning of cells, especially those in the brain. Glucose is primarily obtained through the digestion of carbohydrates, which are broken down into simple sugars. The level of glucose in the blood, known as glycemia, is regulated by hormones such as insulin and glucagon. Elevated glucose levels may indicate diabetes, while very low levels can cause hypoglycemia, both of which can be harmful to health.\n"
          ]
        }
      ],
      "source": [
        "prompt_explicacao = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Me explique sobre o assunto de forma resumida\"),\n",
        "        (\"user\", \"{topico_geral}\")\n",
        "    ]\n",
        ")\n",
        "prompt_traducao = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Voc√™ √© perfeitamente fluente em ingl√™s e preciso que traduza esse texto para mim.\"),\n",
        "        (\"user\", \"{topico_geral}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_explicacao | llm | StrOutputParser() | prompt_traducao | llm | StrOutputParser()\n",
        "print(chain.invoke({\"topico_geral\": \"glicose\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6eq6BV6V5Uz"
      },
      "source": [
        "### Exerc√≠cio 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "HY4s4j8NV5Uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e57942-2ca9-4d61-ab8f-7252db6879e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O Cesar School oferece diversos cursos focados em tecnologia e inova√ß√£o. Alguns dos principais cursos incluem:\n",
            "\n",
            "1. **Cursos de P√≥s-Gradua√ß√£o**:\n",
            "   - MBA em Gest√£o de Projetos\n",
            "   - MBA em Inova√ß√£o\n",
            "   - MBA em Marketing Digital\n",
            "   - Mestrado em Engenharia de Computa√ß√£o\n",
            "   - Especializa√ß√£o em Desenvolvimento de Software\n",
            "\n",
            "2. **Cursos de Extens√£o**:\n",
            "   - Cursos de capacita√ß√£o em √°reas como Design, Dados, Intelig√™ncia Artificial, Transforma√ß√£o Digital e Programa√ß√£o.\n",
            "   - Workshops e eventos para atualiza√ß√£o profissional.\n",
            "\n",
            "3. **Cursos de Forma√ß√£o**:\n",
            "   - Forma√ß√£o em √°reas espec√≠ficas, como Desenvolvimento Web, UX/UI Design, Data Science, entre outros.\n",
            "\n",
            "Al√©m disso, o Cesar School promove programas de forma√ß√£o continuada e customizada para empresas e organiza√ß√µes, visando atender √†s demandas do mercado.\n",
            "\n",
            "Para informa√ß√µes mais detalhadas e atualizadas, √© recomend√°vel visitar o site oficial do Cesar School.\n"
          ]
        }
      ],
      "source": [
        "prompt_respostas_cesar = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Voc√™ √© um especialista no Cesar School, de Recife. Responda as perguntas feitas sobre o Cesar.\"),\n",
        "        (\"user\"), \"{pergunta}\"\n",
        "    ]\n",
        ")\n",
        "chain = prompt_respostas_cesar | llm | StrOutputParser()\n",
        "print(chain.invoke({\"pergunta\": \"Quais cursos tem no Cesar?\"}))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}